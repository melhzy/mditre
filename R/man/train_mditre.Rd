% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainer.R
\name{train_mditre}
\alias{train_mditre}
\title{Train MDITRE Model}
\usage{
train_mditre(
  model,
  train_loader,
  val_loader = NULL,
  test_loader = NULL,
  epochs = 500,
  learning_rates = list(),
  temperature_schedule = list(),
  priors = list(),
  use_scheduler = TRUE,
  checkpoint_dir = NULL,
  checkpoint_every = 50,
  early_stopping_patience = NULL,
  device = NULL,
  verbose = TRUE,
  log_every = 10
)
}
\arguments{
\item{model}{MDITRE or MDITREAbun model created with mditre_model() or mditre_abun_model()}

\item{train_loader}{torch dataloader for training data (from create_dataloader())}

\item{val_loader}{torch dataloader for validation data (optional)}

\item{test_loader}{torch dataloader for test data (optional)}

\item{epochs}{Number of training epochs (default: 500)}

\item{learning_rates}{Named list of learning rates for each parameter group:
\itemize{
\item kappa: Learning rate for phylogenetic focus kappa (default: 0.001)
\item eta: Learning rate for phylogenetic focus eta (default: 0.001)
\item time: Learning rate for temporal focus sigma (default: 0.01)
\item mu: Learning rate for temporal focus mu (default: 0.01)
\item thresh: Learning rate for threshold detectors (default: 0.0001)
\item slope: Learning rate for slope detectors (default: 0.00001)
\item alpha: Learning rate for detector selection (default: 0.005)
\item beta: Learning rate for rule selection (default: 0.005)
\item fc: Learning rate for classifier weights (default: 0.001)
\item bias: Learning rate for classifier bias (default: 0.001)
}}

\item{temperature_schedule}{Named list with temperature annealing parameters:
\itemize{
\item k_bc_max: Max temperature for binary concrete (default: 10)
\item k_bc_min: Min temperature for binary concrete (default: 1)
\item k_otu_max: Max temperature for OTU selection (default: 1000)
\item k_otu_min: Min temperature for OTU selection (default: 100)
\item k_time_max: Max temperature for time window (default: 10)
\item k_time_min: Min temperature for time window (default: 1)
\item k_thresh_max: Max temperature for threshold (default: 1000)
\item k_thresh_min: Min temperature for threshold (default: 100)
\item k_slope_max: Max temperature for slope (default: 10000)
\item k_slope_min: Min temperature for slope (default: 1000)
}}

\item{priors}{Named list with prior parameters:
\itemize{
\item z_mean: Mean active detectors per rule (default: 1)
\item z_var: Variance of active detectors per rule (default: 5)
\item z_r_mean: Mean active rules (default: 1)
\item z_r_var: Variance of active rules (default: 5)
\item w_var: Normal prior variance on weights (default: 1e5)
}}

\item{use_scheduler}{Whether to use cosine annealing learning rate scheduler (default: TRUE)}

\item{checkpoint_dir}{Directory to save model checkpoints (default: NULL, no saving)}

\item{checkpoint_every}{Save checkpoint every N epochs (default: 50)}

\item{early_stopping_patience}{Stop training if validation loss doesn't improve for N epochs (default: NULL, disabled)}

\item{device}{torch device ("cuda" or "cpu", default: auto-detect)}

\item{verbose}{Print training progress (default: TRUE)}

\item{log_every}{Print log every N epochs (default: 10)}
}
\value{
A list containing:
\itemize{
\item model: Trained model
\item history: Training history (losses, metrics per epoch)
\item best_model_state: State dict of best model (by validation loss)
\item best_epoch: Epoch with best validation performance
}
}
\description{
Main training function for MDITRE models. Implements the complete training
loop with optimizer configuration, learning rate scheduling, validation,
and model checkpointing.
}
\examples{
\dontrun{
# Load data
library(torch)
mditre_data <- phyloseq_to_mditre(ps_data, "Subject", "Time", "Disease")
split_data <- split_train_test(mditre_data, test_fraction = 0.2)
train_loader <- create_dataloader(split_data$train, batch_size = 16, shuffle = TRUE)
val_loader <- create_dataloader(split_data$test, batch_size = 16, shuffle = FALSE)

# Create model
model <- mditre_model(
  num_rules = 5,
  num_otus = mditre_data$metadata$n_otus,
  num_time = mditre_data$metadata$n_timepoints,
  dist = mditre_data$phylo_dist
)

# Train model
result <- train_mditre(
  model = model,
  train_loader = train_loader,
  val_loader = val_loader,
  epochs = 200,
  checkpoint_dir = "checkpoints/",
  early_stopping_patience = 20
)

# Access results
trained_model <- result$model
history <- result$history
plot(history$train_loss, type = "l", main = "Training Loss")
}

}
