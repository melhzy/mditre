% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation.R
\name{compare_models}
\alias{compare_models}
\title{Compare Multiple Models}
\usage{
compare_models(
  mditre_data,
  model_configs,
  test_fraction = 0.2,
  seed = 42,
  verbose = TRUE
)
}
\arguments{
\item{mditre_data}{MDITRE data object}

\item{model_configs}{List of model configurations (each with model_params and train_params)}

\item{test_fraction}{Fraction of data for testing (default: 0.2)}

\item{seed}{Random seed (default: 42)}

\item{verbose}{Print progress (default: TRUE)}
}
\value{
List with:
\itemize{
\item results: Results for each model
\item comparison: Data frame comparing metrics
\item best_model_idx: Index of best model (by F1 score)
}
}
\description{
Compare performance of multiple MDITRE models with different configurations.
}
\examples{
\dontrun{
# Define model configurations
configs <- list(
  list(
    name = "MDITRE_5rules",
    model_params = list(num_rules = 5, num_otus = n_otus, 
                       num_time = n_time, dist = dist),
    train_params = list(epochs = 200)
  ),
  list(
    name = "MDITRE_10rules",
    model_params = list(num_rules = 10, num_otus = n_otus,
                       num_time = n_time, dist = dist),
    train_params = list(epochs = 200)
  )
)

# Compare models
comparison <- compare_models(mditre_data, configs)
print(comparison$comparison)
}

}
