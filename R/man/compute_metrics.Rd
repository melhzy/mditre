% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation.R
\name{compute_metrics}
\alias{compute_metrics}
\title{Compute Classification Metrics}
\usage{
compute_metrics(predictions, labels, threshold = 0.5)
}
\arguments{
\item{predictions}{Predicted probabilities (numeric vector or torch tensor)}

\item{labels}{True binary labels (numeric vector or torch tensor)}

\item{threshold}{Classification threshold (default: 0.5)}
}
\value{
Named list with metrics:
\itemize{
\item accuracy: Overall accuracy
\item precision: Positive predictive value
\item recall: Sensitivity / True positive rate
\item f1: F1 score (harmonic mean of precision and recall)
\item sensitivity: Same as recall
\item specificity: True negative rate
\item auc: Area under ROC curve
\item confusion_matrix: 2x2 confusion matrix
}
}
\description{
Compute comprehensive classification metrics including accuracy, precision,
recall, F1 score, sensitivity, specificity, and AUC-ROC.
}
\examples{
\dontrun{
# Generate example predictions
predictions <- c(0.9, 0.8, 0.3, 0.2, 0.7, 0.1)
labels <- c(1, 1, 0, 0, 1, 0)

# Compute metrics
metrics <- compute_metrics(predictions, labels)
print(metrics$f1)
print(metrics$auc)
}

}
